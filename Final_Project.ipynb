{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Renders images as PNG\n",
    "%matplotlib inline \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV into dataframe. Dataset is from https://www.kaggle.com/datasets/kazanova/sentiment140/code.\n",
    "DATASET_COLUMNS=['target','ids','date','query','user','text']\n",
    "DATASET_ENCODING = 'ISO-8859-1'\n",
    "df = pd.read_csv('Twitter.tweets.csv',\\\n",
    "    encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows how many rows and columns are in the dataset\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "\n",
    "Here I will be cleaning the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows and add then add them to new dataframe.\n",
    "#df_nd=df.drop_duplicates(subset=['text'],keep='last').copy()\n",
    "#print(len(df_nd))\n",
    "#print(df_nd.head())\n",
    "#print(df_nd.tail())\n",
    "#df_nd.to_csv('df_noduplicates.csv',header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on dataset description Positive sentiments are marked as 4\n",
    "# The following reassigns the values to be more binary.\n",
    "\n",
    "df['target'] = df['target'].replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will visualise the number of values mark as positive and negative. \n",
    "sentiment = {0:\"Negative\", 1:\"Positive\"}\n",
    "print(df.target.apply(lambda x: sentiment[x]).value_counts())\n",
    "df.target.apply(lambda x: sentiment[x]).value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will reduce dataset to one hundred thousand values\n",
    "# this also reduces the time it takes for the model to run.\n",
    "pos_sentiment = df[df['target'] == 1]\n",
    "neg_sentiment = df[df['target'] == 0]\n",
    "\n",
    "df_pos = pos_sentiment.iloc[:int(50000)]\n",
    "df_neg = neg_sentiment.iloc[:int(50000)]\n",
    "\n",
    "df = pd.concat([df_pos, df_neg])\n",
    "\n",
    "print(len(df))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I am only going to be analysing the sentiment of the tweet. I will only be using the target and text columns.\n",
    "new_df = df[['target', 'text']]\n",
    "new_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am importing the dependies required for analysis. The following dependies will be imported.\n",
    "# Natural language tool kit(NLTK) - This will help with natural language understanding. This also parameters necessary for tokenization and lammatizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I will be importing stopwords\n",
    "# These will remove unnecessary words from the text column\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use this cell to remove words in the stopwords dictionary.\n",
    "#stop_words.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to Lower case\n",
    "# This makes it easier to remove stopwords\n",
    "\n",
    "new_df['text']=new_df['text'].str.lower()\n",
    "new_df['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing wordnet lemmantizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert stringified list to list\n",
    "def strg_list_to_list(strg_list):\n",
    " return strg_list.strip(\"[]\").replace(\"'\",\"\").replace('\"',\"\").replace(\",\",\"\").split() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepprocessing Task\n",
    "Here I will be removing the following from the dataframe\n",
    "1. Retweet ...label\n",
    "2. VIDEO: label\n",
    "3. Hyperlink\n",
    "4. Twitter Handle\n",
    "5. Escape Sequence\n",
    "6. Extra Spaces\n",
    "7. Short words less than 3 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short words\n",
    "\n",
    "new_df['text'] = new_df['text'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pre-tokenization task functions will receive strings as input parametres\n",
    "#and return strings as output\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "def remove_retweet_label(text):\n",
    "  return re.sub('RT @[\\w_]+:','', text)\n",
    "\n",
    "def remove_video_label(text):\n",
    "  return re.sub('VIDEO:','', text)\n",
    "\n",
    "def remove_hyperlink(text):\n",
    "  return re.sub(r'http\\S+','', text) # r=raw \\S=string\n",
    "\n",
    "def remove_twitterhandle(text):\n",
    "  return re.sub('@[A-Za-z0-9_]+(:)?','', text)\n",
    "\n",
    "def remove_escape_sequence(text):\n",
    "  return re.sub(r'\\n','', text)\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "  return  re.sub(r\"\\s+\",\" \", text)  \n",
    "\n",
    "def remove_contraction(text):\n",
    "  return ' '.join([contractions.fix(word) for word in text.split()])\n",
    "  \n",
    "def remove_stopwords(text):\n",
    "  return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def pretokenization_cleaning(text):\n",
    "  text=remove_retweet_label(text)\n",
    "  text=remove_video_label(text)\n",
    "  text=remove_hyperlink(text)\n",
    "  text=remove_twitterhandle(text)\n",
    "  text=remove_escape_sequence(text)\n",
    "  text=remove_extra_spaces(text)  \n",
    "  text=remove_contraction(text)\n",
    "  text=remove_stopwords(text)\n",
    "  return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Tokenization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining tokenizing function\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "def tokenize(text):\n",
    "  tokenizer = TweetTokenizer(reduce_len=True)\n",
    "  return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining Normalizing task using Stemmer\n",
    "import nltk\n",
    "def stemming(unkn_input):\n",
    "  porter = nltk.PorterStemmer()\n",
    "  if (isinstance(unkn_input,list)):\n",
    "    list_input=unkn_input\n",
    "  if (isinstance(unkn_input,str)):\n",
    "    list_input=strg_list_to_list(unkn_input)\n",
    "  list_stemmed=[]\n",
    "  for word in list_input:\n",
    "    word=porter.stem(word)\n",
    "    list_stemmed.append(word)\n",
    "  return \" \".join(list_stemmed) #use this to return a string\n",
    "  #return list_stemmed #use this to return a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining normalizing task using Lemmatizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "#lemmatize requires list input\n",
    "def lemmatize(unkn_input):\n",
    "    if (isinstance(unkn_input,list)):\n",
    "      list_input=unkn_input\n",
    "    if (isinstance(unkn_input,str)):\n",
    "      list_input=strg_list_to_list(unkn_input)\n",
    "    list_sentence = [item.lower() for item in list_input]\n",
    "    nltk_tagged = nltk.pos_tag(list_sentence)  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_pos_tagger(x[1])),nltk_tagged)\n",
    "    \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "        \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Tokenization Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following post-tokenization receives list as input parameter\n",
    "#and returns list as output\n",
    "\n",
    "def remove_punc(list_token): \n",
    "  #print(list_token)\n",
    "  def process(strg_token):\n",
    "    strg_numb ='''0123456789'''\n",
    "    strg_3dots='...'\n",
    "    strg_2dots=\"..\"\n",
    "    strg_punc = '''!()+-[]{}|;:'\"\\,<>./?@#$£%^&*_~“”…‘’'''\n",
    "    strg_output=''\n",
    "    #for idx, char in enumerate(strg_token): \n",
    "    #print(item)\n",
    "    if (len(strg_token)==0): #empty char\n",
    "        strg_output +=''\n",
    "    else:\n",
    "      if (all(char in strg_numb for char in strg_token) or\n",
    "          strg_token[0] in strg_numb): #if char is a number\n",
    "        strg_output +=''\n",
    "      else:\n",
    "        if (len(strg_token)==1 and strg_token in strg_punc): #if char is a single punc\n",
    "          strg_output +=''\n",
    "        else:\n",
    "            if (strg_token[0]=='#'): #if char is hashtag\n",
    "              strg_output +=strg_token.lower()\n",
    "            elif(strg_token==strg_3dots or strg_token==strg_2dots):\n",
    "              strg_output +=''\n",
    "            else: # other than above, char could be part of word,\n",
    "            # e.g key-in\n",
    "              strg_output += strg_token\n",
    "    return strg_output\n",
    "  list_output=[process(token) for token in list_token]\n",
    "  return list_output\n",
    "\n",
    "\n",
    "def remove_empty_item(list_item):\n",
    "  token = [token for token in list_item if len(token)>0]\n",
    "  return token\n",
    "\n",
    "def lowercase_alpha(list_token):\n",
    "  return [token.lower() if (token.isalpha() or token[0]=='#') else token for token in list_token]\n",
    "\n",
    "def posttokenization_cleaning(unkn_input):\n",
    "  list_output=[]\n",
    "  if (isinstance(unkn_input,list)):\n",
    "    list_output=unkn_input\n",
    "  if (isinstance(unkn_input,str)):\n",
    "    list_output=strg_list_to_list(unkn_input)\n",
    "  list_output=remove_punc(list_output)\n",
    "  list_output=remove_empty_item(list_output)\n",
    "  #list_output=lowercase_alpha(list_output)\n",
    "\n",
    "\n",
    "  return (list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling pretokenization_cleaning (list comprehension style)\n",
    "new_df['pretoken']=[pretokenization_cleaning(sentence) for sentence in new_df['text']]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling tokenize (list comprehension style)\n",
    "new_df['token']=[tokenize(sentence) for sentence in new_df['pretoken']]\n",
    "new_df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lammitising the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling stemming (list comprehension style)\n",
    "new_df['stemmed']=[stemming(tokenize(sentence)) for sentence in new_df['pretoken']]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling stemming (list comprehension style)\n",
    "new_df['lemmatized']=[lemmatize(tokenize(word)) for word in new_df['pretoken']]\n",
    "new_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post tokenization cleaning. Here I will be removing numbers, empty tokens, single punctuations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling posttokenization_cleaning (list comprehension style)\n",
    "new_df['posttoken']=[posttokenization_cleaning(list_sentence) for list_sentence in new_df['lemmatized']]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(posttoken):\n",
    "    # Concatenate all tokens into a single string\n",
    "    tokens = [token for sublist in new_df.posttoken for token in sublist]\n",
    "    text = \" \".join(token for token in tokens)\n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud(width=800, height=400, max_words=200, background_color=\"white\").generate(text)\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(new_df['posttoken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_counts = pd.Series(' '.join(new_df['posttoken']).split()).value_counts()\n",
    "#word_counts[:20].plot(kind='bar', figsize=(10,5))\n",
    "#plt.title('20 Most Frequent Words')\n",
    "#plt.xlabel('Words')\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Feature and Label\n",
    "\n",
    "X = new_df.posttoken\n",
    "y = new_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data into Train and Test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF : Extracting Tf-idf features\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train = [str(x) for x in X_train]\n",
    "X_val = [str(x) for x in X_val]\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.90, min_df=0.02, max_features=1000, stop_words='english')\n",
    "\n",
    "tfidf.fit(list(X_train) + list(X_val))\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict_tfid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Naive bayes using Tf-idf features\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb = nb.predict(X_val_tfidf)\n",
    "print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred_nb, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict_tfid['Multinomial Naive Bayes(Tfid)'] = accuracy_score(y_pred_nb, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a confusion matrix.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cf_matrix_model = confusion_matrix(y_val, y_pred_nb)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix_model.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix_model.flatten()/np.sum(cf_matrix_model)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix_model, annot=labels, fmt='', cmap='binary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
